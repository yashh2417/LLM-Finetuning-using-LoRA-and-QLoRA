{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RyWRh3toMm6R",
    "outputId": "949e0ba4-7469-40d4-fac1-869afbbde617"
   },
   "outputs": [],
   "source": [
    "!pip install peft\n",
    "!pip install transformers\n",
    "!pip install bitsandBytes\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7cfzYCDNKue",
    "outputId": "554894c8-9d46-44b9-88c9-9e8abaa3e91c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9MfXPoINmiL"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from transformers import LlamaTokenizer\n",
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8qoyq1YkO7Va",
    "outputId": "75f96410-efcb-4176-d9c9-0a6c748408c9"
   },
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176,
     "referenced_widgets": [
      "a59d329ef4014e41b7208384191961d3",
      "d66f2d651ce04df8a917f7a72f7397ef",
      "cd9e81d9bae2499d815499de563f17cf",
      "2f81c0f4be044eb7b022b1c8d784ffc5",
      "298be7ecefc44d3ca5c0552666cfb6c8",
      "0c5241b1755e42e087b9314781a7ac64",
      "6885224d10cb4a5fbd026e61424f6da7",
      "1cf1209f212048a4be56bc47ad5d6442",
      "8916396e563b42eabbee0cc6a6d080ce",
      "174f58cfc905475cbf7b2ba18bbb3a05",
      "c6a4758153d54291b8777965c3e6d9e8"
     ]
    },
    "id": "Damv-iBxPM5t",
    "outputId": "eff7637f-3592-4f9a-e918-7c2673af152b"
   },
   "outputs": [],
   "source": [
    "base_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3wYyVB6Q2c7",
    "outputId": "9dd1eef7-ac83-41a7-82c8-76cd14dc86eb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_name = \"BhagavadGita.txt\"\n",
    "file_path = os.path.abspath(file_name)\n",
    "\n",
    "print(\"Full path:\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "b22cac181ba546b18f3cafbb7b5c131f",
      "137fd5625b374a46a5ef542a54e7d0a1",
      "ef893da1d1a74fd68ea0c6d4385031df",
      "8565c6794a3b4e4da70264444532ed7b",
      "d4ff89b2bb51445889384c172e05686c",
      "89253819d740495bb84a6e8585e04793",
      "b549e81c283d44469d64043f3207b16a",
      "76d3085092104077a62b4c222a0ddc22",
      "aba651aa10884f209ec0dd670a8d83b3",
      "df4b3bfb54b448039657f62d6bd0a293",
      "2bfab241f0394cbc92c2e5f6a4621ec5",
      "9880aa1fcae845df85315b7e3e765b84",
      "238560b854f64a138e537d5e0d9e5ae0",
      "c3b4163330a940a2b871e2f9fb65c1bb",
      "daa82e5d00c94485be076a31a0b46438",
      "197d6cc46c00410c97d1240248ec2e6f",
      "9a11526f25414426ab4ed8d626d66433",
      "404734a0ff32441388d9645b91fda6ad",
      "d10225a1aec04c60866d6324e83be20f",
      "52bca5976a734b52a723e12336a202d6",
      "b258cdf2af8d4181879b18fe6836ab7d",
      "7b16e374e153412f8f98eea30bde4c2b",
      "bf52bb57773049e2927c220ef22cfd73",
      "c72f83f30d86487b94796cb2ca036dec",
      "28279fc7c32b4b93ae265a6892228e81",
      "b631ed9430614e5aa61ff699c1a77829",
      "42a4ac948a9048e78dc033b1f683123d",
      "735c2376fe6f4a939b6e938ceacd9520",
      "732a26bccf4e497ebf2f76dc63b2196d",
      "dbbbf09099534c33bb2fc90712d2a6b7",
      "da1f08a4f2734a74bebfbf477e9d5097",
      "c01be06c4e374fa485d4eba2fc08b4a2",
      "741171ba6cc64213a162c575212e750f",
      "ceb6d99d8c414428bbc279b4bd70aa73",
      "a0ce0b5dee9b48fb9e59eecc8ccbce2b",
      "20617c3849fc45cfa5bdb5075126725e",
      "3736648baa754bc3831264af9c523bd8",
      "a5fb1412b67e4b89a49ac445449d7e01",
      "d6b68062e970443caa1d17faa38c9974",
      "298e1c8bfaa94b9b99d8faf905fbfd3c",
      "950bed240fa84ebc9ccf57b749a29808",
      "142131a0585046639ba53f041bda45e5",
      "fb98e8e06eee4e08848aee6f60380ac2"
     ]
    },
    "id": "BkWCvMUhTJDK",
    "outputId": "e448cea6-3da4-4c58-ad3f-d6f666291b95"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Features, Value\n",
    "\n",
    "features = Features({'text': Value('string')})\n",
    "\n",
    "train_dataset = load_dataset(\n",
    "    \"text\",\n",
    "    data_files={\"train\": [\"/content/data/BhagavadGita.txt\", \"/content/data/siva_puranam.txt\"]},\n",
    "    split=\"train\",\n",
    "    features=features\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "BY1tL8JMV-Ou",
    "outputId": "d4c4ee60-9de9-42bd-ba13-a1582fe0f00c"
   },
   "outputs": [],
   "source": [
    "train_dataset[\"text\"][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWwc9UOCW-Oy"
   },
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False, trust_remote_code=True, add_eos_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SpO42GlXnSX"
   },
   "outputs": [],
   "source": [
    "if tokenizer.pad is None:\n",
    "  tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXoW5JRqYacP"
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset=[]\n",
    "for phrase in train_dataset:\n",
    "    tokenized_train_dataset.append(tokenizer(phrase[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggIVCNTVYvOv",
    "outputId": "3005c51f-fd39-4c22-ed28-e336a285ccd3"
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "avNT6a24Yzx6",
    "outputId": "56fc2021-58cb-4f75-fe51-6829218c0ca0"
   },
   "outputs": [],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M0wrxxHiZFdH",
    "outputId": "26451b44-75e3-4d70-bab3-3926c36f2039"
   },
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model=prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "OzejL2_RZp6e",
    "outputId": "b9899b9f-55f7-43df-f630-155c38df962b"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "# Fix: Ensure tokenizer has a pad_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # or use tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Optional: Resize model embeddings if you've added a new pad token\n",
    "# model.resize_token_embeddings(len(tokenizer))  # Only if you added new token\n",
    "\n",
    "# Setup Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./finetunedModel\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        num_train_epochs=5,\n",
    "        learning_rate=1e-4,\n",
    "        max_steps=100,\n",
    "        fp16=False,\n",
    "        optim=\"paged_adamw_8bit\",  # Use standard \"adamw_torch\" if \"paged_adamw_8bit\" is unsupported\n",
    "        logging_dir=\"./log\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_steps=50,\n",
    "        logging_steps=20,\n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "# Optional: Prevent warning for use_cache during training\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "da9eb804ba7b45d0a2046bb6dcb6a3db",
      "1b929023f52648519a8566748c618128",
      "76e36272c40749c5a3fc24f0bb488309",
      "04a3219bb6424b849d01f274b392cf94",
      "beb4d0b6e4b64761b0405e4750c56de3",
      "e013a8bed54b471893a6bd43bf594a87",
      "db37de1c6b2b43369754a317f8d9a33c",
      "d65ee9afd3f940108fc0d60ebacdf68a",
      "d670168649cd444687f32838eccdf603",
      "44d02932ccc349f2b58b0b879c8903d4",
      "9e4a179b7abe4e75a630c85f7af5c04c"
     ]
    },
    "id": "njObl-X-b5AS",
    "outputId": "f0510d9a-234b-43f8-fc56-98461c56f959"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import LlamaTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "nf4Config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False, trust_remote_code=True, add_eos_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=nf4Config, device_map=\"auto\", trust_remote_code=True, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8MzrHjODgh36",
    "outputId": "218c73ae-2908-4b3a-e74d-0f34c0184c89"
   },
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False, trust_remote_code=True, add_eos_token=True)\n",
    "\n",
    "modelFinetuned = PeftModel.from_pretrained(model, \"finetunedModel/checkpoint-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q4WwmrPMhGKV",
    "outputId": "bee3574d-b0b8-4751-cf8c-146a4a0b74fc"
   },
   "outputs": [],
   "source": [
    "user_question = \"what is mentioned in shiva puran about lord shiva?\"\n",
    "\n",
    "eval_prompt = f\"Question: {user_question} just answer this question accurately and concisely. \\n\"\n",
    "\n",
    "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "modelFinetuned.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  print(tokenizer.decode(modelFinetuned.generate(**promptTokenized, max_new_tokens=1024)[0], skip_special_tokens=True))\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WfM5D0sOisp2",
    "outputId": "36856873-2e56-4a77-fdd6-0cdacc75e85f"
   },
   "outputs": [],
   "source": [
    "user_question = \"what is the role of lord krishna in bhagavad gita?\"\n",
    "\n",
    "eval_prompt = f\"Question: {user_question} just answer this question accurately and concisely. \\n\"\n",
    "\n",
    "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "modelFinetuned.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  print(tokenizer.decode(modelFinetuned.generate(**promptTokenized, max_new_tokens=1024)[0], skip_special_tokens=True))\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zGotfnLRj_0F"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
